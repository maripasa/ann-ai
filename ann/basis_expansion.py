import marimo

__generated_with = "0.17.2"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    return mo, np, plt, train_test_split


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # Basis Expansion Models

    Basis function expansion models transform the original input into a higher-dimensional space using a set of functions (basis functions) and then fit a linear model in this space.  
    This allows linear models to capture **nonlinear relationships**.

    ## 1. Polynomial Basis Functions
    - Transform input x into powers of x: [1, x, x^2, x^3, ..., x^d]
    - The model becomes: y ≈ w0 + w1*x + w2*x^2 + ... + wd*x^d
    - Useful for smooth nonlinear trends
    - Can overfit if degree d is too high

    ## 2. Radial Basis Functions (RBF)
    - Transform input using radial functions centered at predefined points
    - Creates local, bell-shaped features
    - Captures localized patterns; commonly used in RBF networks

    ## Summary
    - Basis expansion allows linear models to **approximate nonlinear functions**
    - Choice of basis (polynomial, radial, etc.) affects flexibility and generalization
    - Often used in **regression, classification, and kernel methods**
    """
    )
    return


@app.cell
def _(np):
    class LinearRegressor:
        def __init__(self):
            self.w = None

        def _augment_with_bias(self, X):
            X = X[:, np.newaxis] if X.ndim == 1 else X
            return np.hstack([np.ones((X.shape[0], 1)), X])

        def train(self, X, y):
            Xn = self._augment_with_bias(X)
            self.w, *_ = np.linalg.lstsq(Xn, y, rcond=None) # .solve was giving problems thx to nearly singular matrix generated by non orthogonal polynomials. It happens.

        def predict(self, X):
            Xn = self._augment_with_bias(X)
            return Xn @ self.w

        def evaluate(self, X, y):
            y_pred = self.predict(X)
            mse = np.mean((y - y_pred) ** 2)
            return mse
    return (LinearRegressor,)


@app.cell
def _(LinearRegressor, np, plt, train_test_split):
    """
    Default Linear Model
    We plan here to predict a non linear function, in this case, 0.5*x² + 0.5 * x + 7
    """
    np.random.seed(42)

    n = 100

    ep = np.random.normal(0,1,n)
    x = np.random.uniform(-10, 10, n)
    y = 0.5 * x**2 + 0.4 * x + 7 + ep

    fig, axs = plt.subplots()

    X_train, X_test, y_train, y_test = train_test_split(
        x, y, test_size=0.2, random_state=42
    )

    lg = LinearRegressor()
    lg.train(X_train, y_train)

    w_x = np.linspace(-10, 10, n)
    w_y = lg.predict(w_x)

    axs.set_aspect('equal')

    axs.plot(w_x, w_y, "r-")
    axs.scatter(X_train, y_train, c="blue")
    axs.scatter(X_test, y_test, c="green")
    # Just realized that there is no reason for the split, but it happens, and I won't fix it, if this were golang it wouldn't even compile. I blame it all on python.
    # It actually would compile, since I pass it to axs.scatter. I still blame python, you can't stop me.

    """
    As you can see, the model cannot predict the function reliably, since it's nature is linear.
    """

    plt.show()
    return X_test, X_train, w_x, x, y_test, y_train


@app.cell
def _(LinearRegressor, X_test, X_train, np, plt, w_x, y_test, y_train):
    """
    Polynomial Basis Functions
    """

    def poly(x, d):
        return [x**power for power in range(d+1)]

    d = 2

    X_train_poly = np.asarray([poly(xi, d) for xi in X_train])

    lg_poly = LinearRegressor()
    lg_poly.train(X_train_poly, y_train)

    w_x_poly = np.asarray([poly(wi, d) for wi in w_x])
    w_y_poly = lg_poly.predict(w_x_poly)


    fig2, axs2 = plt.subplots()
    axs2.set_aspect('equal')
    axs2.plot(w_x, w_y_poly, "r-")
    axs2.scatter(X_train, y_train, c="blue")
    axs2.scatter(X_test, y_test, c="green")

    """
    Using polynomial basis expansion, the linear model can now approximate
    a quadratic function
    """

    plt.show()
    return


@app.cell
def _(LinearRegressor, X_test, X_train, np, plt, w_x, x, y_test, y_train):
    """
    RBF
    """

    gamma = 0.2  # Usually you should choose a gamma value best fitted for your dataset.
                # gamma ~ 1 / (2 * sigma^2) where sigma is typical distance between centers.
    n_centers = len(x) // 3  # typically you would choose somewhere between n/5 and n/2. You could also use k-means.
    indices = np.random.choice(len(x), size=n_centers, replace=False)
    centers = x[indices]

    def gaussian(x, centers, gamma):
        X_rbf = []
        for xi in x:
            features = []
            for c in centers:
                # the center calculation is just the sum of the squares
                # the inputs x also should usually be normalized, so f(x) is more stable. But it is what it is:
                features.append(np.exp(-gamma * (xi - c)**2))
            X_rbf.append(features)
        return np.array(X_rbf)

    X_train_rbf = gaussian(X_train, centers, gamma)

    lg_rbf = LinearRegressor()
    lg_rbf.train(X_train_rbf, y_train)

    w_x_rbf = gaussian(w_x, centers, gamma)
    w_y_rbf = lg_rbf.predict(w_x_rbf)

    fig3, axs3 = plt.subplots()
    axs3.set_aspect('equal')
    axs3.plot(w_x, w_y_rbf, "r-")
    axs3.scatter(X_train, y_train, c="blue")
    axs3.scatter(X_test, y_test, c="green")

    """
    And now, thx to rbf, lg can predict the function! (nearly, but the inputs can be fiddled with)
    """

    plt.show()
    return


if __name__ == "__main__":
    app.run()
